behaviors:
  PF:                       #https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md
  
    trainer_type: ppo       #The type of trainer to use: ppo, sac, or poca.
    hyperparameters:
      batch_size: 512 #128          #If you are using continuous actions, this value should be large (on the order of 1000s).Typical range : (Continuous - PPO): 512 - 5120; (Continuous - SAC): 128 - 1024; (Discrete, PPO & SAC): 32 - 512.
      buffer_size: 4096 #2048       # Typical range: PPO: 2048 - 409600; SAC: 50000 - 1000000
      learning_rate: 0.0003         # 1e-5 - 1e-3 | This should typically be decreased if training is unstable, and the reward does not consistently increase.
      beta: 0.01                    # Strength of the entropy regularization, which makes the policy "more random." This ensures that agents properly explore the action space during training
      epsilon: 0.5                  # Influences how rapidly the policy can evolve during training. Setting this value small will result in more stable updates, but will also slow the training process.
      lambd: 0.95                   #Regularization parameter (lambda) used when calculating the Generalized Advantage Estimate |  high bias = low | high varience = high values
      num_epoch: 8 #3               #low value => slower learning Typical range(3-10) the larger the barch size, the larger this can increased
      learning_rate_schedule: linear #Determines how beta changes over time. linear | constant 
      num_epoch: 3                  # [3-10] ensure more stable updates, at the cost of slower learning.
      
    network_settings:
      normalize: true
      hidden_units: 512 #128        #how many units / layer
      num_layers: 4                 #how many layers inbetween input-output layers , more complex problem could be easier with CNN
      vis_encode_type: simple       # nature_cnn resnet match3 fully_connected
      conditioning_type: hyper      #Note that when using hyper the number of parameters of the network increases greatly. Therefore, it is recommended to reduce the number of hidden_units when using this conditioning_type
      memory:
        sequence_length: 32 #8         #Typical range: 4 - 128
        memory_size: 128 #64            #Typical range: 32 - 256
        
    reward_signals:
      extrinsic:
        gamma: 0.99 #0.99    # 0.8 - 0.995 Discount factor for future rewards coming from the environment
        strength: 1.0   # Factor by which to multiply the reward given by the environment. Typical ranges will vary depending on the reward signal.
      curiosity:        # encourage the agent to explore the envnoment
        gamma: 0.99     # 0.8 - 0.995
        strength: 0.003  # 0.001 - 0.1 Magnitude of the curiosity reward generated by the intrinsic curiosity module.
        network_settings:
          hidden_units: 256
        learning_rate: 0.0005 #This should typically be decreased if training is unstable, and the curiosity loss is unstable
      rnd:
        gamma: 0.99
        strength: 0.03
        network_settings:
          hidden_units: 64
          num_layers: 3
        learning_rate: 0.0003
      gail:
        strength: 0.1
        gamma: 0.99
        demo_path: Project/Demos/PFdemo.demo
    
    behavioral_cloning:
        demo_path: Project/Demos/PFdemo.demo
        strength: 0.1
        steps: 150000   
        
    keep_checkpoints: 5
    checkpoint_interval: 1000000
    max_steps: 10000000
    time_horizon: 256       #This number should be large enough to capture all the important behavior within a sequence of an agent's actions.
    summary_freq: 10000
    threaded: false          #Allow environments to step while updating the model. For best performance, leave setting to false when using self-play
